{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones de pérdida\n",
    "\n",
    "La función de pérdida es un componente fundamental del proceso de entrenamiento de una RN. Mide la distancia entre la salida obtenida por el modelo (para una determinada entrada) y la salida deseada. Este valor es utilizado por el optimizador del modelo para realizar el cálculo de los gradientes del modelo, al final de cada *step* del proceso de entrenamiento.\n",
    "\n",
    "Existen dos categorías de funciones de pérdida:\n",
    "\n",
    "- Regresión: Son útiles para procesos de optimización cuyo objetivo es predecir un valor numérico en un intervalo continuo (ej. la probabilidad de lluvia).\n",
    "- Clasificación: Son útiles para tareas en las que el objetivo es predecir un valor entre varios posibles valores categóricos (ej. la especie de un animal que aparece en una fotografía).\n",
    "\n",
    "En Pytorch, existen implementaciones para varias funciones de optimización populares. Antes de ver algunos ejemplos, veamos cómo podemos realizar el cálculo de una función de pérdida sencilla utilizando operaciones básicas sobre tensores.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.14000000059604645\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Genera dos salidas de la red neuronal sintéticas\n",
    "output1 = torch.tensor([1.0, 2.0, 3.0, 4.0])  # Sample output 1\n",
    "output2 = torch.tensor([0.5, 1.5, 2.5, 3.5])  # Sample output 2\n",
    "\n",
    "# Genera un vector de valores objetivo (ground truth)\n",
    "target = torch.tensor([0.8, 1.7, 2.9, 3.8])\n",
    "\n",
    "# Calcula la diferencia al cuadrado entre las salidas y los valores objetivo\n",
    "squared_diff1 = torch.pow(output1 - target, 2)\n",
    "squared_diff2 = torch.pow(output2 - target, 2)\n",
    "\n",
    "# Calcula el error cuadrático medio (MSE) entre las salidas y los valores objetivo\n",
    "loss = torch.mean(squared_diff1) + torch.mean(squared_diff2)\n",
    "print(\"MSE Loss:\", loss.item())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exite una función en Pytorch que implementa el MSE, la cual se puede utilizar de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.14000000059604645\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "output1 = torch.tensor([1.0, 2.0, 3.0, 4.0])  # Salida de ejemplo 1\n",
    "output2 = torch.tensor([0.5, 1.5, 2.5, 3.5])  # Salida de ejemplo 2\n",
    "target = torch.tensor([0.8, 1.7, 2.9, 3.8])\n",
    "\n",
    "# Calcula el error cuadrático medio (MSE) entre las salidas y los valores objetivo usando la función MSELoss\n",
    "criterion = torch.nn.MSELoss()\n",
    "loss = criterion(output1, target) + criterion(output2, target)\n",
    "print(\"MSE Loss:\", loss.item())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De aquí en adelante vamos a usar ejemplos en los que siempre se usan funciones que implementan cálculos de funciones de pérdida populares en Pytorch. \n",
    "\n",
    "Siguiendo, dentro de la categoría de funciones de pérdida regresivas, vemos ahora un ejemplo, basado en el miso caso de uso de la función de pérdida L1Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Loss: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Calcula el error cuadrático medio (L1) entre las salidas y los valores objetivo usando la función L1Loss\n",
    "criterion = torch.nn.L1Loss()\n",
    "loss = criterion(output1, target) + criterion(output2, target)\n",
    "print(\"L1 Loss:\", loss.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que en este caso el valor de pérdida obtenido es mucho más alto. La diferencia es que MSELoss usaba el cuadrado de las diferencias, mientras L1Loss utiliza para su cálculo el valor absoluto de la diferencia.\n",
    "\n",
    "Veamos ahora, una reproducción del mismo ejemplo, utilizando la función de pérdida regresiva Mean Bias Error (MBE). En este caso, no hay ninguna implementación para Pytorch, así que utilizamos una implementación hecha por nosotros mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2000, 0.3000, 0.1000, 0.2000])\n",
      "tensor([-0.3000, -0.2000, -0.4000, -0.3000])\n",
      "MBE Loss: -0.05000001937150955\n"
     ]
    }
   ],
   "source": [
    "# Calcula el Mean Bias Error (MBE) entre las salidas y los valores objetivo \n",
    "diff1 = output1 - target\n",
    "diff2 = output2 - target\n",
    "print(diff1)\n",
    "print(diff2)\n",
    "loss = torch.mean(diff1) + torch.mean(diff2)\n",
    "print(\"MBE Loss:\", loss.item()/2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad\n",
    "\n",
    "Dadas las salidas reales y desadas anteriores: *outpu1*, *output2*, *target*, calcula una función de pérdida con la siguiente formulación.\n",
    "\n",
    "FPP = ∑ |output1 - target| + ∑ |output2 - target|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pon tu código aquí"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de pérdida categóricas\n",
    "\n",
    "Vamos a explorar ahora varios ejemplos de funciones de pérdida categóricas. En este caso, se abordan problemas en los que la salida del modelo es un vector de probabilidades. Cada elemento de dicho vector, mide la probabilidad de que la entrada se clasifique en la categoría correspondiente. \n",
    "\n",
    "Un ejemplo sencillo, sería una RN para la cual se predice el número contenido en imágenes del conjunto de datos CIFAR-10. La salida es un vector de 10 elementos, donde el primer elemento contiene la probabilidad de que el número en la imagen sea un 0, el segundo elemento la probabilidad de que sea un 1, etc...\n",
    "\n",
    "Las funciones de pérdida de este tipo deben capturar la diferencia entre la clase prevista y las probabilidades contenidas en vector de salida obtenido de la inferencia del modelo. La clase prevista asigna un 100% de probabilidad a la clase correcta y 0 a todas las demás, por lo tanto, las funciones de pérdida deben capturar cuánto se alejan las probabilidades del modelo de esta predicción perfecta.\n",
    "\n",
    "Las funciones de pérdida categóricas tratan de capturar esta diferencia. Una de ellas es *SVM Loss* y vemos ahora un ejemplo sobre esta función cuya fórmula simplificada es SVM Loss = ∑ max(0, 1 - y_i * f(x_i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Loss: 2.7039999961853027\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Predicción y objetivo de ejemplo\n",
    "predictions = torch.tensor([0.85, 0.12, 0.08, 0.01, 0, 0, 0.23, 0, 0.4 ,0.2]) \n",
    "targets = torch.tensor([1, 0, 0, 0, 0, 0, 0, 0, 0 ,0])\n",
    "\n",
    "# Cálculo de la pérdida SVM\n",
    "loss = torch.mean(torch.max(torch.zeros_like(targets), 1 - targets * predictions))\n",
    "print(\"SVM Loss:\", loss.item())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de Pytorch *MultiLabelMaginLoss* proporciona una implementación de esta métrica en Pytorch. Veamos el mismo ejemplo usando esta función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Loss: 2.7039999961853027\n"
     ]
    }
   ],
   "source": [
    "# Cálculo de la pérdida SVM con Pytorch\n",
    "loss = torch.nn.MultiLabelMarginLoss()\n",
    "loss = loss(predictions.unsqueeze(0), targets.unsqueeze(0))\n",
    "print(\"SVM Loss:\", loss.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro ejemplo de función de pérdida categórica es Cross Entropy Likelihood Loss. Es una de las más populares y se implementa en Pytorch con la función *torch.nn.CrossEntropyLoss*. Veamos el mismo ejemplo, usando esta función.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEL Loss: 1.6783099174499512\n"
     ]
    }
   ],
   "source": [
    "# Cálculo de la pérdida CEL\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "loss = loss(predictions.unsqueeze(0), targets.argmax().unsqueeze(0))\n",
    "print(\"CEL Loss:\", loss.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio\n",
    "\n",
    "Revisa los contenidos del código que se muestra a continuación y prueba alternativamente a utilizar loss_function1, loss_function2 y loss_function3 como funciones de pérdida. \n",
    "\n",
    "Para ello, se tiene que cambiar la línea de código\n",
    "\n",
    "*loss = loss_func1(outputs, labels)*\n",
    "\n",
    "por la otra versión\n",
    "y ajustar la forma de procesar la capa de salida en la pasada *forward*. Ver código comentado.\n",
    "\n",
    "¿Observas diferencias en su eficacia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[1,  2000] loss: 2.192\n",
      "[1,  4000] loss: 1.895\n",
      "[1,  6000] loss: 1.710\n",
      "[1,  8000] loss: 1.600\n",
      "[1, 10000] loss: 1.527\n",
      "[1, 12000] loss: 1.481\n",
      "[2,  2000] loss: 1.413\n",
      "[2,  4000] loss: 1.369\n",
      "[2,  6000] loss: 1.380\n",
      "[2,  8000] loss: 1.324\n",
      "[2, 10000] loss: 1.318\n",
      "[2, 12000] loss: 1.287\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Download and prepare the CIFAR-10 dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Create a neural network model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Capa de salida cuando usamos CrossEntropyLoss\n",
    "        x = self.fc3(x)\n",
    "        # Capa de salida cuando usamos NLLLoss\n",
    "        # x=F.log_softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "net = Net()\n",
    "# Define two loss functions\n",
    "loss_func1 = nn.CrossEntropyLoss()\n",
    "loss_func2 = nn.NLLLoss()\n",
    "\n",
    "# Define an optimizer\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Training the network using CrossEntropyLoss\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_func1(outputs, labels)\n",
    "        # loss = loss_func2(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
