{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado correctamente.\n",
      "Modelo cargado correctamente.\n",
      "Predicciones del modelo cargado: tensor([[-1.6512, -0.9584,  0.3895, -1.2309,  1.6258]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Definir un modelo sencillo\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Crear una instancia del modelo\n",
    "model = Model()\n",
    "\n",
    "# Guardar y cargar el modelo completo\n",
    "torch.save(model, 'modelo_completo.pth')\n",
    "loaded_model = torch.load('modelo_completo.pth')\n",
    "print(\"Modelo completo cargado correctamente.\")\n",
    "\n",
    "# Guardar y cargar el modelo por parámetros\n",
    "torch.save(model.state_dict(), 'modelo_parametros.pth')\n",
    "loaded_model = Model()\n",
    "loaded_model.load_state_dict(torch.load('modelo_parametros.pth'))\n",
    "print(\"Modelo por parámetros cargado correctamente.\")\n",
    "\n",
    "# Guardar y cargar el modelo en formato ONNX\n",
    "dummy_input = torch.randn(1, 10)\n",
    "torch.onnx.export(model, dummy_input, 'modelo.onnx')\n",
    "loaded_model = torch.onnx.load('modelo.onnx')\n",
    "print(\"Modelo en formato ONNX cargado correctamente.\")\n",
    "\n",
    "# Guardar y cargar el modelo en formato HDF5 (utilizando h5py)\n",
    "import h5py\n",
    "h5file = h5py.File('modelo.h5', 'w')\n",
    "torch.save(model.state_dict(), h5file)\n",
    "h5file.close()\n",
    "\n",
    "h5file = h5py.File('modelo.h5', 'r')\n",
    "loaded_model = Model()\n",
    "loaded_model.load_state_dict(torch.load(h5file))\n",
    "h5file.close()\n",
    "print(\"Modelo en formato HDF5 cargado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, se utiliza un modelo sencillo llamado Model que consiste en una capa lineal. Se crea una instancia del modelo.\n",
    "\n",
    "Luego, se muestra cómo guardar y cargar el modelo en diferentes formatos:\n",
    "\n",
    "    Guardar y cargar el modelo completo: Se utiliza torch.save() y torch.load() para guardar y cargar el modelo completo en el archivo \"modelo_completo.pth\". El modelo completo incluye la arquitectura, los parámetros y el estado de optimización.\n",
    "\n",
    "    Guardar y cargar el modelo por parámetros: Se utiliza torch.save() y torch.load() para guardar y cargar solo los parámetros del modelo en el archivo \"modelo_parametros.pth\". En este caso, se crea una instancia vacía del modelo y se carga solo los parámetros guardados.\n",
    "\n",
    "    Guardar y cargar el modelo en formato ONNX: Se utiliza torch.onnx.export() para exportar el modelo en formato ONNX, que es un formato interoperable y portable. Se guarda en el archivo \"modelo.onnx\" y se carga utilizando torch.onnx.load().\n",
    "\n",
    "    Guardar y cargar el modelo en formato HDF5 (utilizando h5py): Se utiliza la biblioteca h5py para guardar y cargar el modelo en formato HDF5. Primero, se crea un archivo HDF5 llamado \"modelo.h5\" y se guarda el estado de los parámetros del modelo. Luego, se carga el modelo utilizando torch.load() y h5py.File().\n",
    "\n",
    "Es importante tener en cuenta que el formato ONNX y el formato HDF5 son útiles cuando se necesita interoperabilidad con otras bibliotecas o sistemas, mientras que el formato de PyTorch (.pth) es más adecuado para la carga y almacenamiento de modelos dentro del ecosistema de PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import h5py\n",
    "\n",
    "# Definir un modelo pre-entrenado\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Guardar el modelo en formato PyTorch (.pth)\n",
    "torch.save(model, 'modelo.pth')\n",
    "print(\"Modelo guardado correctamente en formato PyTorch (.pth)\")\n",
    "\n",
    "# Cargar el modelo en formato PyTorch (.pth)\n",
    "loaded_model = torch.load('modelo.pth')\n",
    "print(\"Modelo cargado correctamente desde formato PyTorch (.pth)\")\n",
    "\n",
    "# Convertir el modelo a formato ONNX (.onnx)\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "onnx_file = 'modelo.onnx'\n",
    "torch.onnx.export(model, dummy_input, onnx_file)\n",
    "print(\"Modelo convertido a formato ONNX (.onnx)\")\n",
    "\n",
    "# Cargar el modelo en formato ONNX (.onnx)\n",
    "loaded_onnx_model = onnx.load(onnx_file)\n",
    "ort_session = ort.InferenceSession(onnx_file)\n",
    "print(\"Modelo cargado correctamente desde formato ONNX (.onnx)\")\n",
    "\n",
    "# Guardar el modelo en formato HDF5 (.h5)\n",
    "h5_file = 'modelo.h5'\n",
    "with h5py.File(h5_file, 'w') as f:\n",
    "    f.create_group('model')\n",
    "    torch.save(model.state_dict(), f['model'])\n",
    "print(\"Modelo guardado correctamente en formato HDF5 (.h5)\")\n",
    "\n",
    "# Cargar el modelo en formato HDF5 (.h5)\n",
    "loaded_h5_model = models.resnet18(pretrained=False)\n",
    "with h5py.File(h5_file, 'r') as f:\n",
    "    state_dict = torch.load(f['model'], map_location=torch.device('cpu'))\n",
    "loaded_h5_model.load_state_dict(state_dict)\n",
    "print(\"Modelo cargado correctamente desde formato HDF5 (.h5)\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, se utiliza un modelo pre-entrenado ResNet-18 de torchvision como ejemplo.\n",
    "\n",
    "El código muestra cómo cargar y almacenar el modelo en los siguientes formatos:\n",
    "\n",
    "    Formato PyTorch (.pth): El modelo pre-entrenado se guarda utilizando torch.save() en el archivo \"modelo.pth\". Luego se carga utilizando torch.load().\n",
    "\n",
    "    Formato ONNX (.onnx): El modelo se convierte a formato ONNX utilizando torch.onnx.export() y se guarda en el archivo \"modelo.onnx\". Luego se carga utilizando onnx.load() y se crea una sesión de inferencia con onnxruntime.InferenceSession().\n",
    "\n",
    "    Formato HDF5 (.h5): El modelo se guarda en formato HDF5 utilizando h5py y se almacena en el archivo \"modelo.h5\". Luego se carga utilizando torch.load() y h5py.File().\n",
    "\n",
    "Es importante tener en cuenta que, al cargar modelos pre-entrenados, es posible que necesites ajustar las rutas de los archivos o realizar algunas adaptaciones adicionales según tu caso específico."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
