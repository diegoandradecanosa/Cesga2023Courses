{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizadores en Pytorch\n",
    "\n",
    "En este  notebook vamos a prestar una especial atención a los optimizadores en Pytorch. \n",
    "\n",
    "Los optimizadores se encuentran agrupados dentro del paquete torch.optim. Su rol dentro del proceso de aprendizaje es integrarse con el mecanismo de autograd de los tensores y proporcionar una implementación de la pasada *backward*. Para ello, tienen que obtener información de cómo de lejos está el modelo de la salida deseada para realizar un cálculo de los gradientes a aplicar sobre los parámetros del modelo. Las funciones de pérdida se tratan de forma separada en otro notebook.\n",
    "\n",
    "Algunas características comunes a los optimizadores de Pytorch son:\n",
    "\n",
    "- Mantienen el estado actual y actualizan los parámetros usando un cálculo de sus gradientes\n",
    "- Tienen una interfaz común que facilita \n",
    "    - Que sean fácilmente intercambiables\n",
    "    - Que se pueden implementar optimizadores ad-hoc\n",
    "- Reciben un iterables con los parámetros aprendibles del modelo\n",
    "    - También reciben otros parámetros, llamados hiperparámetros del optimizador, que permite su configuración (ejs. learning rate, momentum)\n",
    "\n",
    "Métodos principales:\n",
    "\n",
    "- *backward()*: Calcula los gradientes. Se aplica a la función de pérdida, no al optimizador.\n",
    "- *zero_grad()*: Pone a 0 los gradientes del optimizador\n",
    "- *step()*: Aplica los gradientes calculados por *backward()* a los parámetros del modelo\n",
    "\n",
    "La clase base del optimizador es optim.Optimizer y recibe como parámetros:\n",
    "- Un interable que contiene los parámetros a optimizar\n",
    "- Un diccionario con los hiperparámetros del optimizador (lr, momentum, etc...)\n",
    "\n",
    "A continuación iremos viendo ejemplos ilustrativos del uso de los optimizadores más comunes: SGD, RMSProp, Adagrad, Adam, Adadelta.\n",
    "\n",
    "Empecemos por Stochastic Gradiente Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Época 1, Pérdida: 1.7768517238709627\n",
      "Época 2, Pérdida: 1.3932885832493873\n",
      "Época 3, Pérdida: 1.2550515693319424\n",
      "Época 4, Pérdida: 1.158853273943562\n",
      "Época 5, Pérdida: 1.0884042653586248\n",
      "Época 6, Pérdida: 1.0276772107004815\n",
      "Época 7, Pérdida: 0.973789001745946\n",
      "Época 8, Pérdida: 0.9205946205445873\n",
      "Época 9, Pérdida: 0.8784698215134613\n",
      "Época 10, Pérdida: 0.8438497624738747\n"
     ]
    }
   ],
   "source": [
    "# Importamos las bibliotecas necesarias\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Definimos las transformaciones para los conjuntos de datos\n",
    "transform = transforms.Compose([\n",
    "    # Transformamos las imágenes a tensores\n",
    "    transforms.ToTensor(),\n",
    "    # Normalizamos los tensores con media 0.5 y desviación estándar 0.5\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  \n",
    "])\n",
    "\n",
    "# Cargamos los conjuntos de datos CIFAR-10\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Definimos una red convolucional simple\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)  # 3 canales de entrada para RGB, 6 de salida, kernel de 5x5\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)  # 10 clases de salida para CIFAR-10\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "\n",
    "# Especificamos la función de pérdida y el optimizador\n",
    "# En CrossEntropyLoss, el mejor valor de la función de pérdida es 0\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# SGD: Stochastic Gradient Descent\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, foreach=False, nesterov=False)\n",
    "\n",
    "# Finalmente, entrenamos el modelo\n",
    "# Durante 10 épocas\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    # Para cada lote de datos\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Obtenemos las entradas y las etiquetas del lote del conjunto de entrenamiento\n",
    "        inputs, labels = data\n",
    "        # Reiniciamos los gradientes\n",
    "        optimizer.zero_grad()\n",
    "        # Hacemos una pasada hacia adelante\n",
    "        outputs = model(inputs)\n",
    "        # Calculamos la pérdida\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Hacemos una pasada hacia atrás\n",
    "        loss.backward()\n",
    "        # Actualizamos los parámetros\n",
    "        optimizer.step()\n",
    "        # Imprimimos estadísticas\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f'Época {epoch+1}, Pérdida: {running_loss/len(trainloader)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda anterior, podemos hacer distintas ejecuciones cambiando los valores de los parámetros de la celda de construcción del optimizador:\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, foreach=False, nesterov=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
