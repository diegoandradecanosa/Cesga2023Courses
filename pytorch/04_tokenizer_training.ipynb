{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MOsHUjgdIrIW",
    "outputId": "f84a093e-147f-470e-aad9-80fb51193c8e"
   },
   "outputs": [],
   "source": [
    "#! pip install datasets transformers[sentencepiece]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HFASsisvIrIb"
   },
   "source": [
    "Si est√°s abriendo este cuaderno de forma local, aseg√∫rate de tener instalada la √∫ltima versi√≥n de Datasets y una instalaci√≥n de origen de Transformers en tu entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import send_example_telemetry\n",
    "\n",
    "send_example_telemetry(\"tokenizer_training_notebook\", framework=\"none\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## Carga del dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenando tu propio tokenizer desde cero"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este cuaderno, veremos varias formas de entrenar tu propio tokenizer desde cero en un corpus dado, para que luego puedas usarlo para entrenar un modelo de lenguaje desde cero.\n",
    "\n",
    "¬øPor qu√© necesitar√≠as *entrenar* un tokenizer? Esto se debe a que los modelos Transformer a menudo utilizan algoritmos de tokenizaci√≥n de subpalabras, y necesitan ser entrenados para identificar las partes de las palabras que suelen estar presentes en el corpus que est√°s utilizando. Te recomendamos que eches un vistazo al [cap√≠tulo de tokenizaci√≥n](https://huggingface.co/course/chapter2/4?fw=pt) del curso de Hugging Face para obtener una introducci√≥n general sobre los tokenizers, y al [resumen de tokenizers](https://huggingface.co/transformers/tokenizer_summary.html) para conocer las diferencias entre los algoritmos de tokenizaci√≥n de subpalabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitaremos textos para entrenar nuestro tokenizer. Utilizaremos la biblioteca [ü§ó Datasets](https://github.com/huggingface/datasets) para descargar nuestros datos de texto, lo cual se puede hacer f√°cilmente con la funci√≥n `load_dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejemplo, utilizaremos Wikitext-2 (que contiene 4,5 MB de textos, por lo que el entrenamiento ser√° r√°pido para nuestro ejemplo), pero puedes utilizar cualquier conjunto de datos que desees (y en cualquier idioma, excepto ingl√©s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/roberto.lopez/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos echar un vistazo al conjunto de datos, que tiene 36,718 textos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 36718\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para acceder a un elemento, simplemente tenemos que proporcionar su √≠ndice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' = Valkyria Chronicles III = \\n'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambi√©n podemos acceder a un fragmento directamente, en cuyo caso obtenemos un diccionario con la clave `\"text\"` y una lista de textos como valor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['',\n",
       "  ' = Valkyria Chronicles III = \\n',\n",
       "  '',\n",
       "  ' Senj≈ç no Valkyria 3 : Unrecorded Chronicles ( Japanese : Êà¶Â†¥„ÅÆ„É¥„Ç°„É´„Ç≠„É•„É™„Ç¢3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n',\n",
       "  \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La API para entrenar nuestro tokenizer requerir√° un iterador de lotes de textos, por ejemplo, una lista de listas de textos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "all_texts = [dataset[i : i + batch_size][\"text\"] for i in range(0, len(dataset), batch_size)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evitar cargar todo en memoria (ya que la biblioteca Datasets mantiene los elementos en disco y solo los carga en memoria cuando se solicitan), definimos un iterador de Python. Esto es especialmente √∫til si tienes un conjunto de datos enorme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator():\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i : i + batch_size][\"text\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos c√≥mo podemos utilizar este corpus para entrenar un nuevo tokenizer. Hay dos APIs para hacer esto: la primera utiliza un tokenizer existente y entrenar√° una nueva versi√≥n en tu corpus en una l√≠nea de c√≥digo, la segunda es construir realmente tu tokenizer paso a paso, lo que te permite personalizar cada paso."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando un tokenizador ya existente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si deseas entrenar un tokenizer con los mismos algoritmos y par√°metros que uno existente, simplemente puedes utilizar la API `train_new_from_iterator`. Por ejemplo, vamos a entrenar una nueva versi√≥n del tokenizador GPT-2 en Wikitext-2 utilizando el mismo algoritmo de tokenizaci√≥n.\n",
    "\n",
    "Primero necesitamos cargar el tokenizador que queremos usar como modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬°Aseg√∫rate de elegir un tokenizador de versi√≥n *fast* (respaldado por la biblioteca ü§ó Tokenizers) o de lo contrario, el resto del cuaderno no se ejecutar√° correctamente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, alimentamos el corpus de entrenamiento (ya sea la lista de listas o el iterador que definimos anteriormente) al m√©todo `train_new_from_iterator`. Tambi√©n debemos especificar el tama√±o del vocabulario que queremos usar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=25000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬°Y eso es todo! El entrenamiento es muy r√°pido gracias a la biblioteca ü§ó Tokenizers, respaldada por Rust.\n",
    "\n",
    "Ahora tienes un nuevo tokenizer listo para preprocesar tus datos y entrenar un modelo de lenguaje. Puedes proporcionarle textos de entrada como de costumbre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[], [301, 8639, 9504, 3050, 301, 315], [], [4720, 74, 4825, 889, 8639, 491, 529, 672, 6944, 475, 267, 9504, 374, 2809, 529, 10879, 231, 100, 162, 255, 113, 14391, 4046, 113, 4509, 95, 18351, 4509, 256, 4046, 99, 4046, 22234, 96, 19, 264, 6437, 272, 8639, 281, 261, 3518, 2035, 491, 373, 264, 5162, 3305, 290, 344, 8639, 9504, 3050, 2616, 1822, 264, 364, 259, 14059, 1559, 340, 2393, 1527, 737, 1961, 370, 805, 3604, 288, 7577, 14, 54, 782, 337, 261, 4840, 15585, 272, 19958, 284, 1404, 1696, 284, 1822, 264, 385, 364, 261, 1431, 737, 284, 261, 8639, 906, 272, 2531, 1858, 286, 261, 1112, 9658, 281, 14059, 288, 1626, 340, 645, 6556, 344, 520, 14434, 264, 261, 1485, 3436, 7515, 290, 261, 518, 737, 288, 4750, 261, 302, 22039, 302, 264, 259, 21720, 1743, 3836, 5654, 261, 4259, 281, 4742, 490, 724, 261, 3581, 1351, 283, 1114, 579, 952, 4010, 1985, 2563, 288, 453, 2128, 807, 935, 261, 7655, 3836, 302, 2038, 314, 271, 89, 22414, 302, 272, 315], [324, 737, 1022, 1984, 284, 1525, 264, 7663, 610, 259, 1241, 4816, 281, 261, 693, 3654, 326, 8639, 9504, 1243, 272, 1894, 385, 7631, 261, 3684, 2303, 281, 261, 906, 264, 385, 534, 9638, 5354, 16654, 1030, 264, 844, 344, 1878, 261, 737, 667, 10407, 1315, 337, 906, 727, 3210, 383, 272, 13353, 8814, 8187, 2591, 6086, 74, 298, 288, 7508, 10103, 17447, 304, 11550, 9013, 920, 1898, 403, 1445, 22645, 264, 1071, 359, 8639, 9504, 1243, 2499, 21197, 5400, 19526, 5224, 272, 303, 1241, 990, 281, 3839, 8713, 261, 3418, 272, 324, 737, 331, 83, 2574, 3535, 321, 8351, 370, 1073, 331, 78, 272, 315]], 'attention_mask': [[], [1, 1, 1, 1, 1, 1], [], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer(dataset[:5][\"text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes guardarlo localmente con el m√©todo `save_pretrained`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('my-new-tokenizer/tokenizer_config.json',\n",
       " 'my-new-tokenizer/special_tokens_map.json',\n",
       " 'my-new-tokenizer/vocab.json',\n",
       " 'my-new-tokenizer/merges.txt',\n",
       " 'my-new-tokenizer/added_tokens.json',\n",
       " 'my-new-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer.save_pretrained(\"my-new-tokenizer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O incluso subirlo al [Hugging Face Hub](https://huggingface.co/models) para usar ese nuevo tokenizer desde cualquier lugar. Aseg√∫rate de tener tu token de autenticaci√≥n almacenado ejecutando `huggingface-cli login` en una terminal o ejecutando la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5face7debefc433f9f813969d6d0e4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos casi listos, tambi√©n es necesario que tengas instalado `git lfs`. Puedes hacerlo directamente desde este cuaderno descomentando las siguientes celdas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/robertoLC/my-new-shiny-tokenizer/commit/27c1ed9a5d5b2b6f886c6f8a268507ff9ce9e972', commit_message='Upload tokenizer', commit_description='', oid='27c1ed9a5d5b2b6f886c6f8a268507ff9ce9e972', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer.push_to_hub(\"my-new-shiny-tokenizer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tokenizer ahora se puede volver a cargar en esta m√°quina con el siguiente c√≥digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = new_tokenizer.from_pretrained(\"my-new-tokenizer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O desde cualquier lugar utilizando el ID del repositorio, que es tu espacio de nombres seguido de una barra diagonal y el nombre que le diste en el m√©todo `push_to_hub`, por ejemplo:\n",
    "\n",
    "```python\n",
    "tok = new_tokenizer.from_pretrained(\"sgugger/my-new-shiny-tokenizer\")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, si deseas crear y entrenar un nuevo tokenizer que no se parezca a nada existente, deber√°s construirlo desde cero utilizando la biblioteca ü§ó Tokenizers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construyendo el tokenizer desde cero"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprender c√≥mo construir tu tokenizer desde cero, debemos adentrarnos un poco m√°s en la biblioteca ü§ó Tokenizers y en la pipeline de tokenizaci√≥n. Esta pipeline consta de varios pasos:\n",
    "\n",
    "- **Normalizaci√≥n**: Ejecuta todas las transformaciones iniciales sobre la cadena de entrada inicial. Por ejemplo, cuando necesitas convertir un texto en min√∫sculas, eliminar espacios o incluso aplicar uno de los procesos comunes de normalizaci√≥n Unicode, agregar√°s un Normalizer.\n",
    "- **Pre-tokenizaci√≥n**: Se encarga de dividir la cadena de entrada inicial. Esta es la parte que decide d√≥nde y c√≥mo presegmentar la cadena original. El ejemplo m√°s simple ser√≠a dividir la cadena por espacios.\n",
    "- **Modelo**: Se encarga de descubrir y generar todos los subtokens. Esta es la parte que se puede entrenar y que depende realmente de tus datos de entrada.\n",
    "- **Post-Procesamiento**: Proporciona funciones de construcci√≥n avanzadas para ser compatibles con algunos de los modelos m√°s avanzados basados en Transformers. Por ejemplo, para BERT, envolver√≠a la oraci√≥n tokenizada con los tokens [CLS] y [SEP].\n",
    "\n",
    "Y para revertir el proceso:\n",
    "\n",
    "- **Decodificaci√≥n**: Se encarga de mapear nuevamente una entrada tokenizada a la cadena original. El decodificador se elige generalmente seg√∫n el `PreTokenizer` que se utiliz√≥ anteriormente.\n",
    "\n",
    "Para entrenar el modelo, la biblioteca ü§ó Tokenizers proporciona una clase `Trainer` que utilizaremos.\n",
    "\n",
    "Todos estos componentes se pueden combinar para crear pipelines de tokenizaci√≥n funcionales. Para darte algunos ejemplos, mostraremos tres pipelines completas aqu√≠: c√≥mo replicar GPT-2, BERT y T5 (que te dar√°n ejemplos de tokenizadores BPE, WordPiece y Unigram)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPiece model like BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos c√≥mo podemos crear un tokenizador WordPiece similar al utilizado para entrenar BERT. El primer paso es crear un `Tokenizer` con un modelo `WordPiece` vac√≠o:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwargs option unl_token\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(models.WordPiece(unl_token=\"[UNK]\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este `tokenizer` a√∫n no est√° listo para el entrenamiento. Tenemos que agregar algunos pasos de preprocesamiento: la normalizaci√≥n (que es opcional) y el pre-tokenizador, que dividir√° las entradas en fragmentos que llamaremos palabras. Los tokens formar√°n parte de esas palabras (pero no pueden ser m√°s grandes que eso).\n",
    "\n",
    "En el caso de BERT, la normalizaci√≥n implica convertir todo a min√∫sculas. Como BERT es un modelo muy popular, tiene su propio normalizador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si deseas personalizarlo, puedes utilizar los bloques existentes y componerlos en una secuencia: aqu√≠, por ejemplo, convertimos todo a min√∫sculas, aplicamos la normalizaci√≥n NFD y eliminamos los acentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambi√©n existe un `BertPreTokenizer` que podemos utilizar directamente. Realiza una pre-tokenizaci√≥n utilizando espacios en blanco y puntuaci√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que con el normalizador, podemos combinar varios pre-tokenizadores en una secuencia (`Sequence`). Si queremos echar un vistazo r√°pido a c√≥mo se preprocesan las entradas, podemos llamar al m√©todo `pre_tokenize_str`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', (0, 4)),\n",
       " ('is', (5, 7)),\n",
       " ('an', (8, 10)),\n",
       " ('example', (11, 18)),\n",
       " ('!', (18, 19))]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"This is an example!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten en cuenta que el pre-tokenizador no solo divide el texto en palabras, sino que tambi√©n conserva los desplazamientos, es decir, el inicio y el final de cada una de esas palabras dentro del texto original. Esto permitir√° que el tokenizador final pueda asignar cada token a la parte del texto de la que proviene (una caracter√≠stica que utilizamos para tareas de respuesta a preguntas o clasificaci√≥n de tokens).\n",
    "\n",
    "Ahora podemos entrenar nuestro tokenizador (aunque el pipeline no est√© completamente terminado, necesitaremos un tokenizador entrenado para construir el postprocesador). Para ello, utilizamos un `WordPieceTrainer`. Lo importante es recordar pasar los tokens especiales al entrenador, ya que no se ver√°n en el corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar el tokenizador, el m√©todo es similar a lo que usamos anteriormente: podemos pasar archivos de texto o un iterador de lotes de textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que el tokenizador est√° entrenado, podemos definir el post-procesador: necesitamos agregar el token CLS al principio y el token SEP al final (para oraciones individuales) o varios tokens SEP (para pares de oraciones). Usaremos `TemplateProcessing` para hacer esto, lo cual requiere conocer los ID de los tokens CLS y SEP (por eso esperamos al entrenamiento).\n",
    "\n",
    "Entonces, primero obtengamos los ID de los dos tokens especiales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y aqu√≠ est√° c√≥mo podemos construir nuestro post-procesador. Tenemos que indicar en la plantilla c√≥mo organizar los tokens especiales con una oraci√≥n (`$A`) o dos oraciones (`$A` y `$B`). Los dos puntos seguidos de un n√∫mero indican el ID del tipo de token a asignar a cada parte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", cls_token_id),\n",
    "        (\"[SEP]\", sep_token_id),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos verificar si obtenemos los resultados esperados codificando un par de oraciones, por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"This is one sentence.\", \"With this one we have a pair.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los tokens para verificar si los tokens especiales se han insertado en los lugares correctos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'one',\n",
       " 'sentence',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'with',\n",
       " 'this',\n",
       " 'one',\n",
       " 'we',\n",
       " 'have',\n",
       " 'a',\n",
       " 'pair',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y podemos verificar que los ID de tipo de token sean correctos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.type_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La √∫ltima pieza en este tokenizador es el decodificador, usamos un decodificador `WordPiece` e indicamos el prefijo especial `##`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que nuestro tokenizador est√° terminado, necesitamos envolverlo dentro de un objeto de Transformers para poder usarlo con la biblioteca Transformers. M√°s espec√≠ficamente, debemos colocarlo dentro de la clase de tokenizador r√°pido correspondiente al modelo que queremos utilizar, aqu√≠ un `BertTokenizerFast`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "new_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y como antes, podemos usar este tokenizador como un tokenizador normal de Transformers y usar los m√©todos `save_pretrained` o `push_to_hub`.\n",
    "\n",
    "Si el tokenizador que est√°s construyendo no coincide con ninguna clase en Transformers porque es muy especial, puedes envolverlo en `PreTrainedTokenizerFast`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE model like GPT-2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos c√≥mo podemos crear un tokenizador BPE similar al utilizado para entrenar GPT-2. El primer paso es crear un objeto `Tokenizer` con un modelo `BPE` vac√≠o:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como antes, tenemos que a√±adir la normalizaci√≥n opcional (que no se utiliza en el caso de GPT-2) y necesitamos especificar un pre-tokenizador antes del entrenamiento. En el caso de GPT-2, se utiliza un pre-tokenizador a nivel de bytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos echar un vistazo r√°pido a c√≥mo se preprocesan las entradas, podemos llamar al m√©todo `pre_tokenize_str`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', (0, 4)),\n",
       " ('ƒ†is', (4, 7)),\n",
       " ('ƒ†an', (7, 10)),\n",
       " ('ƒ†example', (10, 18)),\n",
       " ('!', (18, 19))]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"This is an example!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos el mismo valor predeterminado que en GPT-2 para el espacio de prefijo, por lo que puedes ver que cada palabra tiene un `'ƒ†'` inicial agregado al principio, excepto la primera.\n",
    "\n",
    "¬°Ahora podemos entrenar nuestro tokenizador! Esta vez usamos un `BpeTrainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para completar todo el pipeline, tenemos que incluir el post-procesador y el decodificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y, como antes, terminamos envolvi√©ndolo en un objeto de tokenizer de Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "new_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram model like Albert"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos c√≥mo podemos crear un tokenizador de Unigram, similar al utilizado para entrenar T5. El primer paso es crear un objeto `Tokenizer` con un modelo `Unigram` vac√≠o:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.Unigram())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que antes, debemos agregar la normalizaci√≥n opcional (en este caso, algunas sustituciones y min√∫sculas) y necesitamos especificar un pre-tokenizador antes del entrenamiento. El pre-tokenizador utilizado es un pre-tokenizador de `Metaspace`: reemplaza todos los espacios por un car√°cter especial (por defecto, ‚ñÅ) y luego divide en ese car√°cter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.Replace(\"``\", '\"'), normalizers.Replace(\"''\", '\"'), normalizers.Lowercase()]\n",
    ")\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos ver r√°pidamente c√≥mo se procesan las entradas, podemos llamar al m√©todo `pre_tokenize_str`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‚ñÅThis', (0, 4)), ('‚ñÅis', (4, 7)), ('‚ñÅan', (7, 10)), ('‚ñÅexample!', (10, 19))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"This is an example!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes ver que cada palabra tiene un `‚ñÅ` inicial agregado al principio, como suele hacerse con sentencepiece.\n",
    "\n",
    "¬°Ahora podemos entrenar nuestro tokenizador! Esta vez usamos un `UnigramTrainer`. \"Tenemos que establecer expl√≠citamente el token desconocido en este entrenador, de lo contrario, lo olvidar√° despu√©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = trainers.UnigramTrainer(vocab_size=25000, special_tokens=[\"[CLS]\", \"[SEP]\", \"<unk>\", \"<pad>\", \"[MASK]\"], unk_token=\"<unk>\")\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para finalizar todo el proceso, debemos incluir el post-procesador y el decodificador. El post-procesador es muy similar a lo que vimos con BERT, y el decodificador es simplemente `Metaspace`, al igual que el pre-tokenizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", cls_token_id),\n",
    "        (\"[SEP]\", sep_token_id),\n",
    "    ],\n",
    ")\n",
    "tokenizer.decoder = decoders.Metaspace()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y al igual que antes, terminamos envolviendo esto en un objeto de tokenizador de Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizerFast\n",
    "\n",
    "new_tokenizer = AlbertTokenizerFast(tokenizer_object=tokenizer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Train your tokenizer",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
