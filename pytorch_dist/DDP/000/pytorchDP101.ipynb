{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soporte básico de Paralelismo de Datos en Pytorch\n",
    "\n",
    "Este tutorial explora algunos conceptos básicos relacionados con el soporte nativo para implementar paralelismo de datos en Pytorch.\n",
    "\n",
    "En Pytorch existen dos mecanismos básicos para implementar paralelismo de datos en Pytorch: DataParallel y DistributedDataParallel.\n",
    "\n",
    "El siguiente ejemplo ilustra el uso de DataParallel para explotar el mecanismo DataParallel. Este mecanismo es apto para el uso en un nodo equipado de varios aceleradores (normalmente GPUs) que pueden ser configuradas como trabajadores. Debido a que usa un único proceso Python, y por lo tanto, puede estar afectada por el GIL, no es la opción más recomendable. En cambio, se debería utilizar, incluso en este escenario con un único nodo, la opción más avanzada DistributedDataParallel, que sí genera varios procesos Python que pueden ejecutarse de forma concurrente.\n",
    "\n",
    "Empecemos por un pequeño ejemplo comenado de uso de DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 1.421\n",
      "[2] loss: 0.373\n",
      "[3] loss: 0.073\n",
      "[4] loss: 0.014\n",
      "[5] loss: 0.002\n",
      "[6] loss: 0.000\n",
      "[7] loss: 0.000\n",
      "[8] loss: 0.000\n",
      "[9] loss: 0.000\n",
      "[10] loss: 0.000\n",
      "Finalizó el entrenamiento\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Definición del modelo: uno modelo linear de una sola capa\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "# Generador del conjunto de datos: aleatorio\n",
    "class MyDataset(Dataset):\n",
    "    def __len__(self):\n",
    "        return 1000\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.randn(10), torch.tensor([1.0])  \n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Instanciamos el modelo y lo enviamos a un dispositivo\n",
    "    model = SimpleModel(10, 1)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Pasamos el modelo por el wrapper DataParallel para tener una versión paralela del mismo\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    # Definición del conjunto de datos y el DataLoader\n",
    "    dataset = MyDataset()\n",
    "    dataloader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "    # Definición de la función de pérdida y el optimizador\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Para cada epoch\n",
    "    for epoch in range(10): \n",
    "        running_loss = 0.0\n",
    "        # Para cada sample del conjunto de datos\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print('[%d] loss: %.3f' % (epoch + 1, running_loss / i))\n",
    "\n",
    "    print('Finalizó el entrenamiento')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a hacer el mismo ejemplo pero con el mecanismo de DistributedDataParallel. El código se escribe a un fichero ddp.py y se lanza con el launcher torchrun en el nodo actual. Nótese que solo hemos comentado las líneas de código que diferen respecto a la versión que usa DataParallel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ddp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ddp.py\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import os\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __len__(self):\n",
    "        return 1000\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.randn(10), torch.tensor([1.0])\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Inicialización del backend para las comunicaciones distribuidas (nccl)\n",
    "    dist.init_process_group(backend='nccl')\n",
    "    \n",
    "    # Global rank del proceso actual\n",
    "    global_rank = dist.get_rank()\n",
    "\n",
    "    # El dispositivo usado será igual al global_rank\n",
    "    torch.cuda.set_device(global_rank)\n",
    "\n",
    "    \n",
    "    model = SimpleModel(10, 1).cuda()\n",
    "\n",
    "    # En esta ocasión pasamos el modelo por el Wrapper DDP\n",
    "    ddp_model = DDP(model, device_ids=[global_rank], output_device=global_rank)\n",
    "\n",
    "    # El dataLoader ahora usa un sampler distribuido\n",
    "    dataset = MyDataset()\n",
    "    sampler = DistributedSampler(dataset)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
    "\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = ddp_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print('[%d] loss: %.3f' % (epoch + 1, running_loss / i))\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.892\n",
      "[1] loss: 0.872\n",
      "[2] loss: 0.528\n",
      "[2] loss: 0.543\n",
      "[3] loss: 0.222\n",
      "[3] loss: 0.236\n",
      "[4] loss: 0.101\n",
      "[4] loss: 0.109\n",
      "[5] loss: 0.044\n",
      "[5] loss: 0.041\n",
      "[6] loss: 0.019\n",
      "[6] loss: 0.019\n",
      "[7] loss: 0.007\n",
      "[7] loss: 0.008\n",
      "[8] loss: 0.003\n",
      "[8] loss: 0.003\n",
      "[9] loss: 0.001\n",
      "[9] loss: 0.001\n",
      "[10] loss: 0.001\n",
      "Finished Training\n",
      "[10] loss: 0.001\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 ddp.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorchdist",
   "language": "python",
   "name": "mytorchdist"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
